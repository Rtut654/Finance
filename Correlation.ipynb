{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### The idea of this notebook is to make an analysis of the correlation between companies, make a graph-like visualisation and store the result data. The whole process will allow to make a free choice of companies list, so you can insert your own list of tickers and follow this notebook in order to make visualisation and store .csv results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps: \n",
    "- download tickers of companies\n",
    "- combine them in a single dataframe\n",
    "- calculate correlation between companies\n",
    "- convert table of correlation into pairs of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import csv\n",
    "import time\n",
    "key = '7WKARC4DTBTJVW54'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text_list):\n",
    "    func = lambda x: x != ''\n",
    "    return list(filter(func, text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse wikipedia S&P page to get the latest index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sp500_data():\n",
    "    # get general information about the components of S&P 500 index\n",
    "    sp500_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    sp500_requested = requests.get(sp500_url).text\n",
    "    \n",
    "    sp500_soup = BeautifulSoup(sp500_requested, 'lxml')\n",
    "    sp500_table = sp500_soup.find('table', {'class':'wikitable sortable'})\n",
    "    sp500_tr = sp500_table.findAll('tr')\n",
    "    \n",
    "    # create csv file with following header\n",
    "    columns = remove_spaces(sp500_tr[0].text.split('\\n'))\n",
    "    \n",
    "    with open('data/sp500.csv', 'a', encoding='utf-8') as fp:\n",
    "        # header of csv file\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(columns)  \n",
    "        for row in sp500_tr[1:]:\n",
    "            text_list = row.text.split('\\n')\n",
    "            info = remove_spaces(text_list)\n",
    "            writer.writerow(info)\n",
    "    print('Done!')\n",
    "    time.sleep(13)\n",
    "get_sp500_data()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of SP tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers_list(tickers):\n",
    "    with open('sp500.csv', 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.reader(fp)\n",
    "        for row in reader:\n",
    "            if row!=[]:\n",
    "                tickers.append(row[0])\n",
    "    return tickers[1:]\n",
    "\n",
    "tickers = []            \n",
    "tickers = get_tickers_list(tickers)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download quaotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_quotes(key, tickers, start = None, end = None, save = True):\n",
    "    if not isinstance(tickers, (list, pd.Series)):\n",
    "        tickers = pd.Series(tickers)\n",
    "    for ticker in tickers:\n",
    "        print(ticker)\n",
    "#         try:\n",
    "        ts = TimeSeries(key, output_format='pandas')\n",
    "        core, meta = ts.get_daily(symbol=ticker, outputsize='full')\n",
    "        if start != None:\n",
    "            if end != None:\n",
    "                core = core[start:end]\n",
    "            else:\n",
    "                core = core[start:]         \n",
    "        core.to_csv('data/historical_daily_quotes/{}.csv'.format(ticker))\n",
    "        print(ticker + ' historical quotes were downloaded!')\n",
    "        time.sleep(14)\n",
    "#         except:\n",
    "#             print(ticker + '  not found!')  \n",
    "get_daily_quotes(key,tickers)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine files to get one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date,1. open,2. high,3. low,4. close,5. volume\n",
    "def compile_data(tickers):\n",
    "    main_df = pd.DataFrame()\n",
    "    for count, ticker in enumerate(tickers):\n",
    "        try:\n",
    "            ticker = ticker.split('.')[0]\n",
    "            print(ticker)\n",
    "            df = pd.read_csv('data/historical_daily_quotes1/{}.csv'.format(ticker))\n",
    "            df.set_index('date', inplace=True)\n",
    "\n",
    "            df.rename(columns={'2. high': ticker}, inplace=True)\n",
    "            df.drop(['1. open', '3. low', '4. close', '5. volume'], 1, inplace=True)\n",
    "\n",
    "            if main_df.empty:\n",
    "                main_df = df\n",
    "            else:\n",
    "                main_df = main_df.join(df, how='outer')\n",
    "\n",
    "            if count % 5 == 0:\n",
    "                print(count)\n",
    "                \n",
    "        except:\n",
    "            print('1')\n",
    "    print(main_df.head())\n",
    "    main_df.to_csv('sp500_joined.csv')    \n",
    "    \n",
    "compile_data(tickers)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('sp500_joined.csv')\n",
    "main_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = main_df.iloc[4500:]\n",
    "main_df.reset_index(inplace=True)\n",
    "main_df.drop('index',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate matrix of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main_df\n",
    "df.set_index('date', inplace=True) \n",
    "df_corr = df.pct_change().corr()\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.index = a\n",
    "df_corr.columns =a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "names = sp['Symbol'].tolist()\n",
    "for i, ticker in enumerate(df_corr.index):\n",
    "    if ticker in names:\n",
    "        index = names.index(ticker)\n",
    "        a.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract pairs of correlated companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_corr.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = so[so<1]\n",
    "so = so[abs(so)>0.5]\n",
    "so = so.iloc[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.DataFrame(so)\n",
    "edges['Source'] = None\n",
    "edges['Target'] = None\n",
    "edges['Weight'] = None\n",
    "edges['Index'] = range(so.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(so.index):\n",
    "    edges['Source'][i] = x[0]\n",
    "    edges['Target'][i] = x[1]\n",
    "    edges['Weight'][i] = so[i]  \n",
    "edges = edges.drop(0, 1)  \n",
    "edges.set_index('Index',inplace=True)\n",
    "edges.to_csv('edges1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pd.read_csv('C:\\\\Users\\\\rtut6\\\\Desktop\\\\123\\\\correlation\\\\sp500.csv')\n",
    "sp.drop(columns =['SEC filings','Headquarters Location', 'CIK', 'Founded','Date first added'], axis=1,inplace=True)\n",
    "sp.to_csv('nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
