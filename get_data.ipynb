{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data from web pages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# operate and save data\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# download stock market data\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "# send requests each X seconds\n",
    "import time\n",
    "\n",
    "#\n",
    "import os\n",
    "\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '7WKARC4DTBTJVW54'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text_list):\n",
    "    func = lambda x: x != ''\n",
    "    return list(filter(func, text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_data():\n",
    "    # get general information about the components of S&P 500 index\n",
    "    sp500_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    sp500_requested = requests.get(sp500_url).text\n",
    "    \n",
    "    sp500_soup = BeautifulSoup(sp500_requested, 'lxml')\n",
    "    sp500_table = sp500_soup.find('table', {'class':'wikitable sortable'})\n",
    "    sp500_tr = sp500_table.findAll('tr')\n",
    "    \n",
    "    # create csv file with following header\n",
    "    columns = remove_spaces(sp500_tr[0].text.split('\\n'))\n",
    "    \n",
    "    with open('data/sp500.csv', 'a', encoding='utf-8') as fp:\n",
    "        # header of csv file\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(columns)  \n",
    "        for row in sp500_tr[1:]:\n",
    "            text_list = row.text.split('\\n')\n",
    "            info = remove_spaces(text_list)\n",
    "            writer.writerow(info)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earnings_calendar(tickers): \n",
    "    # check whether parameter passed is of right data type\n",
    "    if not isinstance(tickers, (list, pd.Series)):\n",
    "        tickers = pd.Series(tickers)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        url = 'https://finance.yahoo.com/calendar/earnings?symbol='\n",
    "        request = requests.get(url + ticker).text\n",
    "        soup = BeautifulSoup(request, 'lxml')\n",
    "        table = soup.find('tbody')\n",
    "        if table == None:\n",
    "            print(ticker + ' was not recognized')\n",
    "            continue\n",
    "        table_tr = table.findAll('tr')\n",
    "        \n",
    "        for row in table_tr:\n",
    "            cols = row.find_all('td')\n",
    "            cols = [x.text.strip() for x in cols]\n",
    "            with open('data/historical_earnings_calendar/{}.csv'.format(ticker),\n",
    "                      'a', encoding='utf-8') as fp:\n",
    "                writer = csv.writer(fp)\n",
    "                writer.writerow(cols)\n",
    "        print(ticker + ' earnings calendar were downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_quotes(key, tickers, start = None, end = None, save = True):\n",
    "    if not isinstance(tickers, (list, pd.Series)):\n",
    "        tickers = pd.Series(tickers)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            ts = TimeSeries(key, output_format='pandas')\n",
    "            core, meta = ts.get_daily(symbol=ticker, outputsize='full')\n",
    "            if start != None:\n",
    "                if end != None:\n",
    "                    core = core[start:end]\n",
    "                else:\n",
    "                    core = core[start:]         \n",
    "            core.to_csv('data/historical_daily_quotes/{}.csv'.format(ticker))\n",
    "            print(ticker + ' historical quotes were downloaded!')\n",
    "        except:\n",
    "            print(ticker + '  not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_many_files(key, tickers, start = None, end = None, save = True):\n",
    "    n = 5 \n",
    "    chunks_of_tickers = [tickers[i:i+n] for i in range(0, len(tickers), n)]\n",
    "    \n",
    "    counter = 1\n",
    "    for chunk in chunks_of_tickers:\n",
    "        get_daily_quotes(key, chunk, start, end, save)\n",
    "        print('Chunk ', counter, ' was downloaded!')\n",
    "        counter += 1\n",
    "        time.sleep(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Receive the content of ``url``, parse it as JSON and return the object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_metrics = {\n",
    "    'income_statements': 'financials/income-statement',\n",
    "    'balance_sheets': 'financials/balance-sheet-statement',\n",
    "    'cash_flow': 'financials/cash-flow-statement',\n",
    "    'key_metrics': 'company-key-metrics',\n",
    "    'enterprise_value': 'enterprise-value',\n",
    "    'financial_statement_growth': 'financial-statement-growth'\n",
    "}\n",
    "\n",
    "def get_fundamentals_quarterly(tickers, fundamentals):\n",
    "    if not isinstance(tickers, (list, pd.Series)):\n",
    "        tickers = pd.Series(tickers)\n",
    "    \n",
    "    if not isinstance(fundamentals, dict):\n",
    "        return\n",
    "    \n",
    "    for metrics, address in fundamentals.items():\n",
    "        try:\n",
    "            os.makedirs(\"data/fundamentals/\" + metrics)\n",
    "        except FileExistsError:\n",
    "            # directory already exists\n",
    "            pass\n",
    "    \n",
    "        for ticker in tickers:\n",
    "            print(ticker)\n",
    "            try:\n",
    "                url_base = 'https://financialmodelingprep.com/api/v3/'\n",
    "                url_var = url_base + '{0}/{1}?period=quarter'.format(address, ticker)\n",
    "                data_js = get_jsonparsed_data(url_var)\n",
    "                if not data_js:\n",
    "                    continue\n",
    "                _, col = data_js.keys()\n",
    "                data = pd.DataFrame(data_js[col])\n",
    "                data.to_csv('data/fundamentals/{0}/{1}.csv'.format(metrics, ticker))\n",
    "            except:\n",
    "                print('Error on ' + ticker)\n",
    "                continue\n",
    "        print(metrics + ' data has been downloaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_urls():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    url = 'https://www.investing.com/indices/investing.com-us-500-components'\n",
    "    #\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    request = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(request, 'lxml')\n",
    "    table = soup.find('tbody')\n",
    "    \n",
    "    urls = []\n",
    "    for a in table.find_all('a', href=True):\n",
    "        urls.append(a['href'])\n",
    "    \n",
    "    return urls\n",
    "    \n",
    "def get_estimates(urls):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(\"data/estimates/\")\n",
    "    except FileExistsError:\n",
    "        # directory already exists\n",
    "        pass\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\", \n",
    "                              chrome_options=options)\n",
    "    button_xpath = '//*[@id=\"showMoreEarningsHistory\"]/a'\n",
    "    for url in urls:\n",
    "        driver.get('https://www.investing.com{}-earnings'.format(url))\n",
    "        button = driver.find_element_by_xpath(button_xpath)\n",
    "        \n",
    "        driver.execute_script(\"arguments[0].click();\", button)\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].click();\", button)\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].click();\", button)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        title = soup.find('h1', {'class':'float_lang_base_1 relativeAttr'}).text\n",
    "        ticker = title[title.find(\"(\")+1:title.find(\")\")]\n",
    "         \n",
    "        # there are multiple tables on a page, but we are interested in 1st\n",
    "        class_name = 'genTbl openTbl ecoCalTbl earnings earningsPageTbl'\n",
    "        table = soup.findAll('table', {'class': class_name})\n",
    "        table_rows = table[0].findAll('tr')\n",
    "        \n",
    "        result = pd.DataFrame(columns=['Release Date', 'Period End',\n",
    "                                       'EPS', 'Forecast', 'Revenue', \n",
    "                                       'Forecast'])\n",
    "        for row in table_rows[1:]:\n",
    "            text_list = row.text.split('\\n')\n",
    "            substr = '/\\xa0\\xa0'\n",
    "            chars = len(substr)\n",
    "            new_row = []\n",
    "            for el in text_list:\n",
    "                if el == '':\n",
    "                    continue\n",
    "                if substr in el:\n",
    "                    el = el[chars:]\n",
    "                new_row.append(el)\n",
    "            result.loc[len(result),:] = new_row\n",
    "        \n",
    "        result.to_csv('data/estimates/{}.csv'.format(ticker))\n",
    "        print(url)\n",
    "        print(ticker + ' data has been downloaded!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
